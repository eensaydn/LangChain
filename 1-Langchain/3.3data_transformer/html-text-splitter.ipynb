{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6a5fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo'}, page_content='Foo'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some intro text about Foo.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}, page_content='Bar main section'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}, page_content='Some intro text about Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}, page_content='Bar subsection 1'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}, page_content='Some text about the first subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}, page_content='Bar subsection 2'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}, page_content='Some text about the second subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Baz'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some text about Baz  \\nSome concluding text about Foo')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "\n",
    "html_splitter=HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits=html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6407d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Jump to content  \\nMain menu  \\nMain menu  \\nmove to sidebar  \\nhide  \\nNavigation  \\nMain page  \\nContents  \\nCurrent events  \\nRandom article  \\nAbout Wikipedia  \\nContact us  \\nContribute  \\nHelp  \\nLearn to edit  \\nCommunity portal  \\nRecent changes  \\nUpload file  \\nSpecial pages  \\nSearch  \\nSearch  \\nAppearance  \\nDonate  \\nCreate account  \\nLog in  \\nPersonal tools  \\nDonate  \\nCreate account  \\nLog in  \\nPages for logged out editors  \\nlearn more  \\nContributions  \\nTalk  \\nCentralNotice'),\n",
       " Document(metadata={'Header 2': 'Contents'}, page_content='Contents'),\n",
       " Document(metadata={}, page_content='move to sidebar  \\nhide  \\n(Top)  \\n1  \\nAuthors  \\n2  \\nMethods discussed and introduced  \\n3  \\nHistorical context  \\nToggle Historical context subsection  \\n3.1  \\nPredecessors  \\n3.2  \\nAttention with seq2seq  \\n3.3  \\nParallelizing attention  \\n3.4  \\nAI boom era  \\n4  \\nTraining  \\n5  \\nNotes  \\n6  \\nReferences  \\n7  \\nExternal links  \\nToggle the table of contents  \\nAttention Is All You Need  \\n10 languages  \\nالعربية  \\nCatalà  \\nEspañol  \\n한국어  \\nမြန်မာဘာသာ  \\n日本語  \\nPortuguês  \\nTürkçe  \\nУкраїнська  \\n中文  \\nEdit links  \\nArticle  \\nTalk  \\nEnglish  \\nRead  \\nEdit  \\nView history  \\nTools  \\nTools  \\nmove to sidebar  \\nhide  \\nActions  \\nRead  \\nEdit  \\nView history  \\nGeneral  \\nWhat links here  \\nRelated changes  \\nUpload file  \\nPermanent link  \\nPage information  \\nCite this page  \\nGet shortened URL  \\nDownload QR code  \\nPrint/export  \\nDownload as PDF  \\nPrintable version  \\nIn other projects  \\nWikidata item  \\nAppearance  \\nmove to sidebar  \\nhide  \\nFrom Wikipedia, the free encyclopedia  \\nesi <esi:include src=\"/esitest-fa8a495983347898/content\" />  \\n2017 research paper by Google  \\nAn illustration of main components of the transformer model from the paper  \\n\" \" is a 2017 landmark in authored by eight scientists working at . The paper introduced a new architecture known as the , based on the proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern , and a main contributor to the , as the transformer approach has become the main architecture of a wide variety of AI, such as . At the time, the focus of the research was on improving techniques for , but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like and what is now known as .  \\nAttention Is All You Need  \\n1  \\n[  \\n]  \\n2  \\n[  \\n]  \\n3  \\n[  \\n]  \\nresearch paper  \\nmachine learning  \\nGoogle  \\ndeep learning  \\ntransformer  \\nattention mechanism  \\n4  \\n[  \\n]  \\n5  \\n[  \\n]  \\nartificial intelligence  \\nAI boom  \\nlarge language models  \\n6  \\n[  \\n]  \\n7  \\n[  \\n]  \\nSeq2seq  \\nmachine translation  \\nquestion answering  \\nmultimodal Generative AI  \\n1  \\n[  \\n]  \\nThe paper\\'s title is a reference to the song \" \" by . The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.  \\nAll You Need Is Love  \\nthe Beatles  \\n8  \\n[  \\n]  \\n9  \\n[  \\n]  \\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the franchise. The team was named Team Transformer.  \\nTransformers  \\n8  \\n[  \\n]  \\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and . These convinced the team that the Transformer is a general purpose language model, and not just good for translation.  \\nparsing  \\n9  \\n[  \\n]  \\nAs of 2025, the paper has been cited more than 173,000 times, placing it among top ten most-cited papers of the 21st century.  \\n[update]  \\n10  \\n[  \\n]  \\n11  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Authors'}, page_content='Authors'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nThe authors of the paper are: , , Niki Parmar, Jakob Uszkoreit, , , Łukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The article highlights the group\\'s diversity:  \\nAshish Vaswani  \\nNoam Shazeer  \\nLlion Jones  \\n[ ]  \\nwikidata  \\nAidan Gomez  \\nWired  \\n8  \\n[  \\n]  \\nSix of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.  \\nAfter the paper, each of the authors left Google to join other companies or to found . Several of them expressed feelings of being unable to innovate and expand the Transformer in a direction they want, if they had stayed at Google.  \\nstartups  \\n12  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Methods discussed and introduced'}, page_content='Methods discussed and introduced'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nThe paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern . A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.  \\nLarge Language Models (LLMs)  \\nThe following mechanisms were introduced by the paper as part of the development of the transformer architecture.  \\nScaled dot-product attention & self-attention  \\nThe use of the scaled dot-product attention and self-attention mechanism instead of a or (which rely on recurrence instead) allow for better performance as described in the following paragraph. The paper described the scaled dot-product attention as follows:  \\nRecurrent neural network  \\nLong short-term memory  \\nA  \\nt  \\nt  \\ne  \\nn  \\nt  \\ni  \\no  \\nn  \\n(  \\nQ  \\n,  \\nK  \\n,  \\nV  \\n)  \\n:=  \\ns  \\no  \\nf  \\nt  \\nm  \\na  \\nx  \\n(  \\nQ  \\n× ×  \\nK  \\nT  \\nd  \\nk  \\n)  \\n× ×  \\nV  \\n{\\\\displaystyle {\\\\rm {Attention}}(Q,K,V):={\\\\rm {softmax}}\\\\left({\\\\frac {Q\\\\times K^{T}}{\\\\sqrt {d_{k}}}}\\\\right)\\\\times V}  \\nwhere , , are respectively the query, key, value matrices, and is the dimension of the values.  \\nQ  \\n{\\\\displaystyle Q}  \\nK  \\n{\\\\displaystyle K}  \\nV  \\n{\\\\displaystyle V}  \\nd  \\nk  \\n{\\\\displaystyle d_{k}}  \\nSince the model relies on Query ( , Key ( and Value ( matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors (represented as and initially set to 64 within the paper) in the manner shown above.  \\nQ)  \\nK)  \\nV)  \\nd  \\nk  \\n{\\\\displaystyle d_{k}}  \\nIn the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.  \\nMulti-head attention  \\nIn the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.  \\nBy doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.  \\nPositional encoding  \\nSince the Transformer model is not a seq2seq model and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding. The methods introduced in the paper are discussed below:  \\nP  \\nE  \\n(  \\np  \\no  \\ns  \\n,  \\n2  \\ni  \\n)  \\n=  \\nsin  \\n\\u2061 \\u2061  \\n(  \\np  \\no  \\ns  \\n/  \\n10000  \\n2  \\ni  \\n/  \\nd  \\nm  \\no  \\nd  \\ne  \\nl  \\n)  \\n{\\\\displaystyle PE_{({\\\\rm {pos}},2i)}=\\\\sin({\\\\rm {pos}}/{10000}^{2i/d_{\\\\rm {model}}})}  \\nP  \\nE  \\n(  \\np  \\no  \\ns  \\n,  \\n2  \\ni  \\n+  \\n1  \\n)  \\n=  \\ncos  \\n\\u2061 \\u2061  \\n(  \\np  \\no  \\ns  \\n/  \\n10000  \\n2  \\ni  \\n/  \\nd  \\nm  \\no  \\nd  \\ne  \\nl  \\n)  \\n{\\\\displaystyle PE_{({\\\\rm {pos}},2i+1)}=\\\\cos({\\\\rm {pos}}/{10000}^{2i/d_{\\\\rm {model}}})}  \\nwherein , , correspond to the position of the word, the current dimension index and the dimension of the model respectively. The sine function is used for even indices of the embedding while the cosine function is used for odd indices. The resultant embedding is then added to the word at that corresponding position with respect to the current context window. The paper specifically comments on why this method was chosen describing:  \\np  \\no  \\ns  \\n{\\\\displaystyle {\\\\rm {pos}}}  \\ni  \\n{\\\\displaystyle i}  \\nd  \\nm  \\no  \\nd  \\ne  \\nl  \\n{\\\\displaystyle {d_{\\\\rm {model}}}}  \\nP  \\nE  \\n{\\\\displaystyle PE}  \\n\"We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"  \\n1  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Historical context'}, page_content='Historical context'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}  \\nMain articles: , and  \\nTransformer (deep learning architecture) §\\xa0History  \\nSeq2seq §\\xa0History  \\nSee also:  \\nTimeline of machine learning'),\n",
       " Document(metadata={'Header 3': 'Predecessors'}, page_content='Predecessors'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nFor many years, sequence modelling and generation was done by using plain (RNNs). A well-cited early example was the (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.  \\nrecurrent neural networks  \\nElman network  \\nvanishing-gradient problem  \\nA key breakthrough was (1995), a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an which used neurons that multiply the outputs of other neurons, so-called . Neural networks using multiplicative units were later called or . LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs. Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.  \\nLSTM  \\nnote 1  \\n[  \\n]  \\nattention mechanism  \\nmultiplicative units  \\n13  \\n[  \\n]  \\nsigma-pi networks  \\n14  \\n[  \\n]  \\nhigher-order networks  \\n15  \\n[  \\n]  \\nnote 2  \\n[  \\n]  \\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is in the size of the context window. The linearly scaling controller (1992) learns to compute a weight matrix for further processing depending on the input. One of its two networks has \"fast weights\" or \"dynamic links\" (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer.  \\nquadratic  \\nfast weight  \\n16  \\n[  \\n]  \\n17  \\n[  \\n]  \\n18  \\n[  \\n]  \\n19  \\n[  \\n]  \\n16  \\n[  \\n]  \\n20  \\n[  \\n]  \\n21  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'Attention with seq2seq'}, page_content='Attention with seq2seq'),\n",
       " Document(metadata={'Header 3': 'Attention with seq2seq'}, page_content='[  \\nedit  \\n]  \\nMain article:  \\nSeq2seq §\\xa0History  \\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.  \\n22  \\n[  \\n]  \\n23  \\n[  \\n]  \\nA 380M-parameter model for machine translation uses two (LSTM). Its architecture consists of two parts. The is an LSTM that takes in a sequence of tokens and turns it into a vector. The is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used (GRU) instead of LSTM. Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.  \\nlong short-term memories  \\n23  \\n[  \\n]  \\nencoder  \\ndecoder  \\ngated recurrent units  \\n22  \\n[  \\n]  \\n24  \\n[  \\n]  \\n25  \\n[  \\n]  \\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a -size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.  \\nlast  \\nfixed  \\n26  \\n[  \\n]  \\nThe model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".  \\nRNNsearch  \\nfixed-size  \\n4  \\n[  \\n]  \\nThe relative performances were compared between global (that of ) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.  \\nRNNsearch  \\n27  \\n[  \\n]  \\nIn 2016, was revamped to , which replaced the previous model based on . The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM. It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.  \\nGoogle Translate  \\nGoogle Neural Machine Translation  \\nstatistical machine translation  \\n28  \\n[  \\n]  \\n29  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'Parallelizing attention'}, page_content='Parallelizing attention'),\n",
       " Document(metadata={'Header 3': 'Parallelizing attention'}, page_content='[  \\nedit  \\n]  \\nMain article:  \\nAttention (machine learning) §\\xa0History  \\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to , which prevented them from being accelerated on GPUs. In 2016, applied a self-attention mechanism to , which are easy to parallelize, and achieved result in with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention recurrence is sufficient for language translation, thus the title \"attention is you need\". That hypothesis was against conventional wisdom at the time, and even his father , a well-known computational linguist, was skeptical. In the same year, self-attention (called ) was proposed for LSTMs.  \\nparallelize  \\ndecomposable attention  \\nfeedforward networks  \\nSOTA  \\ntextual entailment  \\n30  \\n[  \\n]  \\nwithout  \\nall  \\n31  \\n[  \\n]  \\nHans Uszkoreit  \\n31  \\n[  \\n]  \\nintra-attention or  \\nintra-sentence attention  \\n32  \\n[  \\n]  \\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \" \" paper. At the time, the focus of the research was on improving for , by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.  \\nAttention is all you need  \\nseq2seq  \\nmachine translation  \\n1  \\n[  \\n]  \\n33  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'AI boom era'}, page_content='AI boom era'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles. Transformer architecture is now used alongside many that contribute to the ongoing .  \\n34  \\n[  \\n]  \\ngenerative models  \\nAI boom  \\nIn language modelling, (2018) was a bi-directional LSTM that produces contextualized , improving upon the line of research from and . It was followed by (2018), an encoder-only Transformer model. In 2019 October, Google started using BERT to process search queries. In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.  \\nELMo  \\nword embeddings  \\nbag of words  \\nword2vec  \\nBERT  \\n35  \\n[  \\n]  \\n36  \\n[  \\n]  \\n37  \\n[  \\n]  \\nStarting in 2018, the OpenAI of decoder-only Transformers became state of the art in . In 2022, a chatbot based on GPT-3, , became unexpectedly popular, triggering a boom around .  \\nGPT series  \\nnatural language generation  \\nChatGPT  \\n38  \\n[  \\n]  \\nlarge language models  \\n39  \\n[  \\n]  \\n40  \\n[  \\n]  \\nSince 2020, Transformers have been applied in modalities beyond text, including the , speech recognition, robotics, and . The vision transformer, in turn, stimulated new developments in . Image and video generators like (2021), (2024), and (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.  \\nvision transformer  \\n41  \\n[  \\n]  \\n42  \\n[  \\n]  \\n43  \\n[  \\n]  \\nmultimodal  \\n44  \\n[  \\n]  \\nconvolutional neural networks  \\n45  \\n[  \\n]  \\nDALL-E  \\nStable Diffusion 3  \\n46  \\n[  \\n]  \\nSora'),\n",
       " Document(metadata={'Header 2': 'Training'}, page_content='Training'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nWhile the primary focus of the paper at the time was to improve machine translation, the paper also discussed the use of the architecture on English , both with limited and large-sized datasets, achieving a high-score without specific tuning for the task indicating the promising nature of the model for use in a wide-variety of general purpose of seq2seq tasks.  \\nConstituency Parsing  \\nDataset  \\nThe English-to-German translation model was trained on the 2014 WMT (Workshop on Statistical Machine Translation) English-German dataset, consisting of nearly 4.5 million sentences derived from TED Talks and high-quality news articles. A separate translation model was trained on the much larger 2014 WMT English-French dataset, consisting of 36 million sentences. Both datasets were encoded with byte-pair encoding.  \\nHardware  \\nThe models were trained using 8 . The base models were trained for 100,000 steps and the big models were trained for 300,000 steps - each step taking about 0.4 seconds to complete for the base models and 1.0 seconds for the big models. The base model trained for a total of 12 hours, and the big model trained for a total of 3.5 days. Both the base and big models outperforms the 2017 state-of-the-art in both English-German and English-French while achieving the comparatively lowest training cost. The estimated computing cost was 0.089 petaFLOP-days.  \\nNVIDIA P100 GPUs  \\n1  \\n[  \\n]  \\n47  \\n[  \\n]  \\nHyperparameters and regularization  \\nFor their 100M-parameter Transformer model, the authors increased the linearly for the first 4000 (warmup) steps and decreased it proportionally to inverse square root of the current step number. Dropout layers were applied to the output of each sub-layer before normalization, the sums of the embeddings, and the positional encodings. The dropout rate was set to 0.1. Label smoothing was applied with a value of 0.1 which \"improves accuracy and BLEU score\".  \\nlearning rate  \\n1  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Notes'}, page_content='Notes'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}  \\n^  \\n(2014) further reduced its complexity.  \\nGated recurrent units  \\n^  \\nSome architectures, such as RWKV or state space models, avoid the issue.'),\n",
       " Document(metadata={'Header 2': 'References'}, page_content='References'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nNewPP limit report\\nParsed by mw‐web.eqiad.main‐c95f674dc‐ns9zq\\nCached time: 20250528141921\\nCache expiry: 2592000\\nReduced expiry: false\\nComplications: [vary‐revision‐sha1, show‐toc]\\nCPU time usage: 0.893 seconds\\nReal time usage: 1.053 seconds\\nPreprocessor visited node count: 3714/1000000\\nRevision size: 15286/2097152 bytes\\nPost‐expand include size: 145983/2097152 bytes\\nTemplate argument size: 2282/2097152 bytes\\nHighest expansion depth: 15/100\\nExpensive parser function count: 8/500\\nUnstrip recursion depth: 1/20\\nUnstrip post‐expand size: 183918/5000000 bytes\\nLua time usage: 0.610/10.000 seconds\\nLua memory usage: 5839921/52428800 bytes\\nNumber of Wikibase entities loaded: 1/500 Transclusion expansion time report (%,ms,calls,template)\\n100.00%  893.864      1 -total\\n 63.06%  563.673      2 Template:Reflist\\n 13.10%  117.115      4 Template:Cite_conference\\n 12.20%  109.036      9 Template:Cite_arXiv\\n 12.11%  108.203      5 Template:Navbox\\n 11.67%  104.331      1 Template:Google_AI\\n  8.23%   73.531      1 Template:Short_description\\n  8.07%   72.092      6 Template:Citation\\n  7.72%   69.037      9 Template:Cite_web\\n  5.00%   44.681      2 Template:Pagetype Saved in parser cache with key enwiki:pcache:75477752:|#|:idhash:canonical and timestamp 20250528141921 and revision id 1288310720. Rendering was triggered because: page-view  \\n^  \\na  \\nb  \\nc  \\nd  \\ne  \\nf  \\n.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}  \\n; ; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; ; Kaiser, Łukasz; Polosukhin, Illia (December 2017). \"Attention is All you Need\". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). . Advances in Neural Information Processing Systems. Vol.\\xa030. Curran Associates, Inc. : .  \\nVaswani, Ashish  \\nShazeer, Noam  \\nGomez, Aidan N  \\n31st Conference on  Neural Information Processing Systems (NIPS)  \\narXiv  \\n1706.03762  \\n^  \\nLove, Julia (10 July 2023). . .  \\n\"AI Researcher Who Helped Write Landmark Paper Is Leaving Google\"  \\nBloomberg News  \\n. Retrieved 2024  \\n1 April  \\n^  \\nGoldman, Sharon (20 March 2024). . .  \\n\" \\'Attention is All You Need\\' creators look beyond Transformers for AI at Nvidia GTC: \\'The world needs something better\\' \"  \\nVentureBeat  \\n. Retrieved 2024  \\n1 April  \\n^  \\na  \\nb  \\nBahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (19 May 2016). \"Neural Machine Translation by Jointly Learning to Align and Translate\". : [ ].  \\narXiv  \\n1409.0473  \\ncs.CL  \\n^  \\nShinde, Gitanjali; Wasatkar, Namrata; Mahalle, Parikshit (6 June 2024). . . p.\\xa075. .  \\nData-Centric Artificial Intelligence for Multidisciplinary Applications  \\nCRC Press  \\nISBN  \\n9781040031131  \\n^  \\nToews, Rob (3 September 2023). . . from the original on 26 September 2023 .  \\n\"Transformers Revolutionized AI. What Will Replace Them?\"  \\nForbes  \\nArchived  \\n. Retrieved 2023  \\n3 December  \\n^  \\nMurgia, Madhumita (23 July 2023). . . from the original on 28 December 2023 .  \\n\"Transformers: the Google scientists who pioneered an AI revolution\"  \\nFinancial Times  \\nArchived  \\n. Retrieved 2024  \\n22 March  \\n^  \\na  \\nb  \\nc  \\nLevy, Steven. . . .  \\n\"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\"  \\nWired  \\nISSN  \\n1059-1028  \\n. Retrieved 2024  \\n20 March  \\n^  \\na  \\nb  \\nMarche, Stephen (23 August 2024). . . .  \\n\"Was Linguistic A.I. Created by Accident?\"  \\nThe New Yorker  \\nISSN  \\n0028-792X  \\n. Retrieved 2024  \\n24 August  \\n^  \\n. . 13 July 2023 – via www.bloomberg.com.  \\n\"Meet the $4 Billion AI Superstars That Google Lost\"  \\nBloomberg  \\n^  \\n. . 15 April 2025 .  \\n\"Exclusive: the most-cited papers of the twenty-first century\"  \\nNature  \\n. Retrieved 2025  \\n18 April  \\n^  \\nMurgia, Madhumita (23 July 2023). . .  \\n\"Transformers: the Google scientists who pioneered an AI revolution\"  \\nFinancial Times  \\n. Retrieved 2025  \\n22 March  \\n^  \\nFeldman, J. A.; Ballard, D. H. (1 July 1982). . . (3): 254. : . .  \\n\"Connectionist models and their properties\"  \\nCognitive Science  \\n6  \\n205–  \\ndoi  \\n10.1016/S0364-0213(82)80001-3  \\nISSN  \\n0364-0213  \\n^  \\nRumelhart, David E.; McClelland, James L.; Hinton, Geoffrey E. (29 July 1987). . Cambridge, Mass: Bradford Books. .  \\nParallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations, Chapter 2  \\n(PDF)  \\nISBN  \\n978-0-262-68053-0  \\n^  \\nGiles, C. Lee; Maxwell, Tom (1 December 1987). . . (23): 4978. : . . .  \\n\"Learning, invariance, and generalization in high-order neural networks\"  \\nApplied Optics  \\n26  \\n4972–  \\ndoi  \\n10.1364/AO.26.004972  \\nISSN  \\n0003-6935  \\nPMID  \\n20523475  \\n^  \\na  \\nb  \\n(1992). . . (1): 139. : . .  \\nSchmidhuber, Jürgen  \\n\"Learning to control fast-weight memories: an alternative to recurrent nets\"  \\n(PDF)  \\nNeural Computation  \\n4  \\n131–  \\ndoi  \\n10.1162/neco.1992.4.1.131  \\nS2CID  \\n16683347  \\n^  \\nChristoph von der Malsburg: The correlation theory of brain function. Internal Report 81-2, MPI Biophysical Chemistry, 1981. See Reprint in Models of Neural Networks II, chapter 2, pages 95–119. Springer, Berlin, 1994.  \\nhttp://cogprints.org/1380/1/vdM_correlation.pdf  \\n^  \\nJerome A. Feldman, \"Dynamic connections in neural networks,\" Biological Cybernetics, vol. 46, no. 1, pp. 27–39, Dec. 1982.  \\n^  \\nHinton, Geoffrey E.; Plaut, David C. (1987). . . .  \\n\"Using Fast Weights to Deblur Old Memories\"  \\nProceedings of the Annual Meeting of the Cognitive Science Society  \\n9  \\n^  \\nKatharopoulos, Angelos; Vyas, Apoorv; Pappas, Nikolaos; Fleuret, François (2020). . . PMLR. pp. 5165.  \\n\"Transformers are RNNs: Fast autoregressive Transformers with linear attention\"  \\nICML 2020  \\n5156–  \\n^  \\nSchlag, Imanol; Irie, Kazuki; (2021). \"Linear Transformers Are Secretly Fast Weight Programmers\". . Springer. pp. 9366.  \\nSchmidhuber, Jürgen  \\nICML 2021  \\n9355–  \\n^  \\na  \\nb  \\nCho, Kyunghyun; van Merriënboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (October 2014). . In Moschitti, Alessandro; Pang, Bo; Daelemans, Walter (eds.). . Doha, Qatar: Association for Computational Linguistics. pp. 1734. : . : .  \\n\"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\"  \\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)  \\n1724–  \\narXiv  \\n1406.1078  \\ndoi  \\n10.3115/v1/D14-1179  \\n^  \\na  \\nb  \\n[first version posted to arXiv on 10 Sep 2014]  \\nSutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (14 December 2014). \"Sequence to sequence learning with neural networks\". : [ ].  \\narXiv  \\n1409.3215  \\ncs.CL  \\n^  \\nChung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". : [ ].  \\narXiv  \\n1412.3555  \\ncs.NE  \\n^  \\nGruber, N.; Jockisch, A. (2020), \"Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?\", , : 40, : , , ,  \\nFrontiers in Artificial Intelligence  \\n3  \\ndoi  \\n10.3389/frai.2020.00040  \\nPMC  \\n7861254  \\nPMID  \\n33733157  \\nS2CID  \\n220252321  \\n^  \\nSutskever, Ilya; Vinyals, Oriol; Le, Quoc V (2014). . . . Curran Associates, Inc. : .  \\n\"Sequence to Sequence Learning with Neural Networks\"  \\nAdvances in Neural Information Processing Systems  \\n27  \\narXiv  \\n1409.3215  \\n^  \\nLuong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (2015). \"Effective Approaches to Attention-based Neural Machine Translation\". : [ ].  \\narXiv  \\n1508.04025  \\ncs.CL  \\n^  \\nWu, Yonghui; et\\xa0al. (1 September 2016). \"Google\\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". : [ ].  \\narXiv  \\n1609.08144  \\ncs.CL  \\n^  \\nLewis-Kraus, Gideon (14 December 2016). . . . Archived from on 24 May 2023 .  \\n\"The Great A.I. Awakening\"  \\nThe New York Times  \\nISSN  \\n0362-4331  \\nthe original  \\n. Retrieved 2023  \\n22 June  \\n^  \\nParikh, Ankur P.; Täckström, Oscar; Das, Dipanjan; Uszkoreit, Jakob (25 September 2016). \"A Decomposable Attention Model for Natural Language Inference\". : [ ].  \\narXiv  \\n1606.01933  \\ncs.CL  \\n^  \\na  \\nb  \\nLevy, Steven. . . . from the original on 20 March 2024 .  \\n\"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\"  \\nWired  \\nISSN  \\n1059-1028  \\nArchived  \\n. Retrieved 2024  \\n6 August  \\n^  \\nCheng, Jianpeng; Dong, Li; Lapata, Mirella (November 2016). . In Su, Jian; Duh, Kevin; Carreras, Xavier (eds.). . Austin, Texas: Association for Computational Linguistics. pp. 561. : .  \\n\"Long Short-Term Memory-Networks for Machine Reading\"  \\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing  \\n551–  \\ndoi  \\n10.18653/v1/D16-1053  \\n^  \\nPeng, Bo; Alcaide, Eric; Anthony, Quentin; Albalak, Alon; Arcadinho, Samuel; Biderman, Stella; Cao, Huanqi; Cheng, Xin; Chung, Michael (10 December 2023), , :  \\nRWKV: Reinventing RNNs for the Transformer Era  \\narXiv  \\n2305.13048  \\n^  \\nMarche, Stephen (23 August 2024). . . .  \\n\"Was Linguistic A.I. Created by Accident?\"  \\nThe New Yorker  \\nISSN  \\n0028-792X  \\n. Retrieved 2024  \\n27 August  \\n^  \\nDevlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". : [ ].  \\narXiv  \\n1810.04805v2  \\ncs.CL  \\n^  \\n. . 15 October 2020 .  \\n\"Google: BERT now used on almost every English query\"  \\nSearch Engine Land  \\n. Retrieved 2020  \\n24 November  \\n^  \\n. .  \\n\"Recent Advances in Google Translate\"  \\nresearch.google  \\n. Retrieved 2024  \\n8 May  \\n^  \\n. .  \\n\"The inside story of how ChatGPT was built from the people who made it\"  \\nMIT Technology Review  \\n. Retrieved 2024  \\n6 August  \\n^  \\n. . 11 June 2018. from the original on 18 March 2023 .  \\n\"Improving language understanding with unsupervised learning\"  \\nopenai.com  \\nArchived  \\n. Retrieved 2023  \\n18 March  \\n^  \\n, OpenAI, 11 June 2018  \\nfinetune-transformer-lm  \\n, retrieved 2023  \\n1 May  \\n^  \\nDosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob (3 June 2021). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". : [ ].  \\narXiv  \\n2010.11929  \\ncs.CV  \\n^  \\nGulati, Anmol; Qin, James; Chiu, Chung-Cheng; Parmar, Niki; Zhang, Yu; Yu, Jiahui; Han, Wei; Wang, Shibo; Zhang, Zhengdong; Wu, Yonghui; Pang, Ruoming (2020). \"Conformer: Convolution-augmented Transformer for Speech Recognition\". : [ ].  \\narXiv  \\n2005.08100  \\neess.AS  \\n^  \\nChen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor (24 June 2021), , :  \\nDecision Transformer: Reinforcement Learning via Sequence Modeling  \\narXiv  \\n2106.01345  \\n^  \\nChoromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Mohiuddin, Afroz (19 November 2022), , :  \\nRethinking Attention with Performers  \\narXiv  \\n2009.14794  \\n^  \\nLiu, Zhuang; Mao, Hanzi; Wu, Chao-Yuan; Feichtenhofer, Christoph; Darrell, Trevor; Xie, Saining (2022). . Conference on Computer Vision and Pattern Recognition. pp. 11986.  \\nA ConvNet for the 2020s  \\n11976–  \\n^  \\nEsser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (5 March 2024), , :  \\nScaling Rectified Flow Transformers for High-Resolution Image Synthesis  \\narXiv  \\n2403.03206  \\n^  \\n. . 9 June 2022 .  \\n\"AI and compute\"  \\nopenai.com  \\n. Retrieved 2025  \\n29 April'),\n",
       " Document(metadata={'Header 2': 'External links'}, page_content='External links'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nA concurrent blog post on Google Research blog.  \\nUszkoreit, Jakob (31 August 2017). . .  \\n\"Transformer: A Novel Neural Network Architecture for Language Understanding\"  \\nresearch.google  \\n. Retrieved 2024  \\n9 August  \\n.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\\\a0 \"}  \\n.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}  \\n.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}  \\nv  \\nt  \\ne  \\nGoogle AI  \\nGoogle  \\nGoogle Brain  \\nGoogle DeepMind  \\nComputer programs  \\nAlphaGo  \\nVersions  \\n(2015)  \\nAlphaGo  \\n(2016)  \\nMaster  \\n(2017)  \\nAlphaGo Zero  \\n(2017)  \\nAlphaZero  \\n(2019)  \\nMuZero  \\nCompetitions  \\n(2015)  \\nFan Hui  \\n(2016)  \\nLee Sedol  \\n(2017)  \\nKe Jie  \\nIn popular culture  \\n(2017)  \\nAlphaGo  \\n(2023)  \\nThe MANIAC  \\nOther  \\n(2018)  \\nAlphaFold  \\n(2019)  \\nAlphaStar  \\n(2023)  \\nAlphaDev  \\n(2024)  \\nAlphaGeometry  \\nMachine learning  \\nNeural networks  \\n(2014)  \\nInception  \\n(2016)  \\nWaveNet  \\n(2017)  \\nMobileNet  \\n(2017)  \\nTransformer  \\n(2019)  \\nEfficientNet  \\n(2022)  \\nGato  \\nOther  \\nQuantum Artificial Intelligence Lab  \\nTensorFlow  \\nTensor Processing Unit  \\nGenerative AI  \\nChatbots  \\n(2016)  \\nAssistant  \\n(2022)  \\nSparrow  \\n(2023)  \\nGemini  \\nModels  \\n(2018)  \\nBERT  \\n(2019)  \\nXLNet  \\n(2019)  \\nT5  \\n(2021)  \\nLaMDA  \\n(2022)  \\nChinchilla  \\n(2022)  \\nPaLM  \\n(2023)  \\nImagen  \\n(2023)  \\nGemini  \\n(2024)  \\nVideoPoet  \\n(2024)  \\nVeo (text-to-video model)  \\nOther  \\n(2022)  \\nDreamBooth  \\n(2023)  \\nNotebookLM  \\n(2024)  \\nVids  \\nSee also  \\n\" \"  \\nAttention Is All You Need  \\nFuture of Go Summit  \\nGenerative pre-trained transformer  \\nGoogle Labs  \\nGoogle Pixel  \\nGoogle Workspace  \\nRobot Constitution  \\nCategory  \\nCommons  \\nRetrieved from \" \"  \\nhttps://en.wikipedia.org/w/index.php?title=Attention_Is_All_You_Need&oldid=1288310720  \\n:  \\nCategories  \\n2017 documents  \\nArtificial intelligence papers  \\nGoogle  \\n2017 in artificial intelligence  \\nHidden categories:  \\nArticles with short description  \\nShort description is different from Wikidata  \\nUse dmy dates from December 2023  \\nArticles containing potentially dated statements from 2025  \\nAll articles containing potentially dated statements  \\nThis page was last edited on 1 May 2025, at 20:36 .  \\n(UTC)  \\nText is available under the ;\\nadditional terms may apply. By using this site, you agree to the and . Wikipedia® is a registered trademark of the , a non-profit organization.  \\nCreative Commons Attribution-ShareAlike 4.0 License  \\nTerms of Use  \\nPrivacy Policy  \\nWikimedia Foundation, Inc.  \\nPrivacy policy  \\nAbout Wikipedia  \\nDisclaimers  \\nContact Wikipedia  \\nCode of Conduct  \\nDevelopers  \\nStatistics  \\nCookie statement  \\nMobile view  \\nSearch  \\nSearch  \\nToggle the table of contents  \\nAttention Is All You Need  \\n10 languages  \\nAdd topic  \\n(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgHostname\":\"mw-web.eqiad.main-c95f674dc-f6vfn\",\"wgBackendResponseTime\":129,\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.893\",\"walltime\":\"1.053\",\"ppvisitednodes\":{\"value\":3714,\"limit\":1000000},\"revisionsize\":{\"value\":15286,\"limit\":2097152},\"postexpandincludesize\":{\"value\":145983,\"limit\":2097152},\"templateargumentsize\":{\"value\":2282,\"limit\":2097152},\"expansiondepth\":{\"value\":15,\"limit\":100},\"expensivefunctioncount\":{\"value\":8,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":183918,\"limit\":5000000},\"entityaccesscount\":{\"value\":1,\"limit\":500},\"timingprofile\":[\"100.00%  893.864      1 -total\",\" 63.06%  563.673      2 Template:Reflist\",\" 13.10%  117.115      4 Template:Cite_conference\",\" 12.20%  109.036      9 Template:Cite_arXiv\",\" 12.11%  108.203      5 Template:Navbox\",\" 11.67%  104.331      1 Template:Google_AI\",\"  8.23%   73.531      1 Template:Short_description\",\"  8.07%   72.092      6 Template:Citation\",\"  7.72%   69.037      9 Template:Cite_web\",\"  5.00%   44.681      2 Template:Pagetype\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.610\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":5839921,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw-web.eqiad.main-c95f674dc-ns9zq\",\"timestamp\":\"20250528141921\",\"ttl\":2592000,\"transientcontent\":false}}});});  \\n{\"@context\":\"https:\\\\/\\\\/schema.org\",\"@type\":\"Article\",\"name\":\"Attention Is All You Need\",\"url\":\"https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Attention_Is_All_You_Need\",\"sameAs\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q30249683\",\"mainEntity\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q30249683\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\/\\\\/www.wikimedia.org\\\\/static\\\\/images\\\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2023-12-04T00:59:48Z\",\"dateModified\":\"2025-05-01T20:36:29Z\",\"image\":\"https:\\\\/\\\\/upload.wikimedia.org\\\\/wikipedia\\\\/commons\\\\/8\\\\/8f\\\\/The-Transformer-model-architecture.png\",\"headline\":\"scientific article published in June 2017\"}')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Attention_Is_All_You_Need\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59c430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
